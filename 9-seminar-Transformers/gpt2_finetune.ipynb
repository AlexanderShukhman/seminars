{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by: c00k1ez (https://github.com/c00k1ez)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue generation with GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer - is a powerful architecture and it performs SOTA solution for most of all seq2seq tasks, like NMT, summarization, and especially for language modeling.\n",
    "On this notebook we are going to work with GPT2. This is a model from OpenAI, which was State-Of-The-Art solution for language modeling in 2019. You can read original blogpost [here](https://openai.com/blog/better-language-models/).  \n",
    "\n",
    "Let's consider an interesting application of GPT2 model - dialogue generation. Describe this task a bit clearer - we have some context, for example, user question and our model have to generate a relevant answer.  \n",
    "How we can train a model for it? First of all, for input, we need to use special tokens to mark context and model answer, like `[CONTEXT] some context [ANSWER] model answer`. Then there are two possible ways:\n",
    "* train it like classic autoregressive LM,\n",
    "* train it like seq2seq LM. Read more [here](https://arxiv.org/abs/1905.03197).  \n",
    "\n",
    "You can read more about GPT2 [here](http://jalammar.github.io/illustrated-gpt2/) and [here](https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8).  \n",
    "[Documentation](https://huggingface.co/transformers/model_doc/gpt2.html) for GPT2.\n",
    "\n",
    "| ![seq2seq lm](https://drive.google.com/uc?export=view&id=1NxS-O0Tto2rcFrALhpUBbywriyKlSTL4) |\n",
    "|:--:| \n",
    "| *seq2seq LM* |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "from src.utils import get_answer, seed_all\n",
    "from src.gpt2.data_parser import Dialogue, DataParser\n",
    "from src.gpt2.dataset import DialogueDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_config = {\n",
    "    'pad_len': 100,\n",
    "    'train_batch_size': 10,\n",
    "    'model_name': 'gpt2',\n",
    "    'lr': 5e-5,\n",
    "    'residual_dropout': 0.1,\n",
    "    'embedding_dropout': 0.1,\n",
    "    'attention_dropout': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the smallest GPT2 model - it has 124M trainable parameters and takes 500mb disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading: 100%|██████████| 548M/548M [06:21<00:00, 1.44MB/s]\n"
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained(params_config['model_name'])\n",
    "config.resid_pdrop = params_config['residual_dropout']\n",
    "config.attn_pdrop = params_config['attention_dropout']\n",
    "config.embd_pdrop = params_config['embedding_dropout']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(params_config['model_name'], config=config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important moment**: we have to add special tokens: `[CONTEXT]` and `[ANSWER]` to tokenizer, then resize model embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few words about tokenizer.\n",
    "GPT2, like some other models, uses [Byte-Pair Encoding](https://leimao.github.io/blog/Byte-Pair-Encoding/) with special tokens in vocabulary.  \n",
    "\n",
    "All tokenizers from `transformers` have unified structure and same methods, so we are going to use a few methods:\n",
    "* `tokenizer.tokenize` to split string unto list of tokens,\n",
    "* `tokenizer.encode` to transform a string into token indexes,\n",
    "* `tokenizer.decode` to transform a list of ids to the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Embedding(50259, 768)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(params_config['model_name'])\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['[CONTEXT]', '[ANSWER]']})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can consider our dataset a bit closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = DataParser('./data/TwitterLowerAsciiCorpus.txt')\n",
    "train, test = parser.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Our dataset contains 8574 context-answer pairs and unique 1983 dialogues\nThere are 10046 unique tokens in dataset and 233751 tokens at all. Notice, that small GPT2 have vocabulary with 50k sub-words.\n\nMost common 10 tokens:\n. : 7196\nĠi : 7186\nĠthe : 4561\nĠto : 4025\nĠyou : 3977\n, : 3602\nĠa : 3373\nĠit : 3218\nĠand : 2602\n's : 2285\n"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print('Our dataset contains {} context-answer pairs and unique {} dialogues'.format(len(parser.all_pairs), len(parser.dialogues)))\n",
    "\n",
    "tokens = []\n",
    "for sample in parser.all_pairs:\n",
    "    sample = tokenizer.tokenize(sample['context'] + ' ' + sample['answer'])\n",
    "    tokens.extend(sample)\n",
    "counter = Counter(tokens)\n",
    "print('There are {} unique tokens in dataset and {} tokens at all. Notice, that small GPT2 have vocabulary with 50k sub-words.'.format(len(counter), sum([v for _,v in dict(counter).items()])))\n",
    "print('')\n",
    "print('Most common 10 tokens:')\n",
    "for token, freq in counter.most_common(10):\n",
    "    print('{} : {}'.format(token, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that we have **really** small dataset for our \"toy\" task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DialogueDataset(train, tokenizer, params_config['pad_len'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, params_config['train_batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Another important moment**: we are using AdamW optimizer from `transformers` package, **not** classic Adam and **not** AdamW from `torch.optim`!  \n",
    "[Blogpost](https://www.fast.ai/2018/07/02/adam-weight-decay/) about AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = transformers.AdamW(model.parameters(), lr=params_config['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw earlier, we have a small dataset, so it is quite hard to get a good result and do not overfit.\n",
    "## How to improve this model results?\n",
    "* Implement `train_test_split` method in `DataParser` class and validation loop to calculate [perplexity](https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3).\n",
    "* Find optimal `residual_dropout`, `embedding_dropout` and `attention_dropout` probabilities.\n",
    "* Now just a previous sentence is used for training like context for answer. You can rewrite `Dialogue.get_pairs` method to sample one, two, three, or more sentences like context for answer.\n",
    "* You can add a bit more regularizations, for example, throw random tokens from the sample, or swap answer and context with a small probability.\n",
    "* Read about [BPE-dropout](https://arxiv.org/abs/1910.13267). It is hard to implement with `transformers`, so you can just read about this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, test_loader, device):\n",
    "    ################### INSERT YOUR CODE HERE ###################\n",
    "        \n",
    "    ################### INSERT YOUR CODE HERE ###################\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, test_loader, optimizer, epoch_num, device, log_interval=100):\n",
    "    losses = []\n",
    "    avg_loss = []\n",
    "    step = 1\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, mask, label = batch['sample'], batch['mask'], batch['label']\n",
    "        input_ids = input_ids.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "        outputs = model(input_ids, attention_mask=mask, labels=label)\n",
    "        loss, logits = outputs[:2]\n",
    "        avg_loss.append(loss.detach().item())\n",
    "        if step % log_interval == 0:\n",
    "            val_loss = sum(avg_loss) / len(avg_loss)\n",
    "            losses.append(val_loss)\n",
    "            avg_loss = []\n",
    "            print('epoch {}\\t[{}/{}]\\tloss = {:.4f}'.format(epoch_num, step, len(loader), val_loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    ep_losses = train_epoch(model, train_loader, None, optimizer, epoch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(torch.device('cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer(\"where are you?\", model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitpytorchenvcondadd1df8879e6648999b744b4f723a1baa",
   "display_name": "Python 3.7.6 64-bit ('pytorch_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}