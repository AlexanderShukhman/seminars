{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitpytorchenvcondadd1df8879e6648999b744b4f723a1baa",
   "display_name": "Python 3.7.6 64-bit ('pytorch_env': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to /home/egor/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from src.data_parser import DataParser\n",
    "from src.tokenizer import Tokenizer\n",
    "from src.dataset import NMTDataset\n",
    "from src.models import NMTModel, Encoder, Decoder\n",
    "from src.utils import seed_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parser = DataParser('./data/rus.txt')\n",
    "eng, ru = data_parser.split_by_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer.build_vocab(eng, './data/vocab_eng.txt')\n",
    "Tokenizer.build_vocab(ru, './data/vocab_ru.txt', threshold=0.7)\n",
    "\n",
    "eng_tokenizer = Tokenizer('eng', './data/vocab_eng.txt')\n",
    "ru_tokenizer = Tokenizer('ru', './data/vocab_ru.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---------- test eng ----------\nI love dogs and cats\n['<BOS>', 'i', 'love', 'dogs', 'and', 'cats', '<EOS>']\n[0, 5, 169, 740, 46, 804, 1]\n['<BOS>', 'i', 'love', 'dogs', 'and', 'cats', '<EOS>']\n---------- test ru ----------\nЯ люблю собак и кошек\n['<BOS>', 'я', 'люблю', 'собак', 'и', 'кошек', '<EOS>']\n[0, 6, 172, 1417, 26, 1575, 1]\n['<BOS>', 'я', 'люблю', 'собак', 'и', 'кошек', '<EOS>']\n"
    }
   ],
   "source": [
    "test_strings = ['I love dogs and cats', 'Я люблю собак и кошек']\n",
    "inform = ['---------- test eng ----------', '---------- test ru ----------']\n",
    "tokenizers = [eng_tokenizer, ru_tokenizer]\n",
    "for test_str, inf, tokenizer in list(zip(test_strings, inform, tokenizers)):\n",
    "    print(inf)\n",
    "    print(test_str)\n",
    "    tokenized = tokenizer.tokenize(test_str)\n",
    "    print(tokenized)\n",
    "    encoded = tokenizer.encode(tokenized)\n",
    "    print(encoded)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dataset': {\n",
    "        'source_pad_len': 100,\n",
    "        'target_pad_len': 100\n",
    "    },\n",
    "    'dataloader': {\n",
    "        'train_bs': 20,\n",
    "        'test_bs': 20\n",
    "    },\n",
    "    'encoder_cfg': {\n",
    "        'vocab_size': eng_tokenizer.get_vocab_size(),\n",
    "        'embedding_size': 256,\n",
    "        'hidden_size': 128\n",
    "    },\n",
    "    'decoder_cfg': {\n",
    "        'vocab_size': ru_tokenizer.get_vocab_size(),\n",
    "        'embedding_size': 256,\n",
    "        'hidden_size': 128\n",
    "    },\n",
    "    'optim': {\n",
    "        'lr': 1e-4\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data_parser.train_test_split(0.9)\n",
    "\n",
    "train_dataset = NMTDataset(train, eng_tokenizer, ru_tokenizer, **config['dataset'])\n",
    "test_dataset = NMTDataset(train, eng_tokenizer, ru_tokenizer, **config['dataset'])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['dataloader']['train_bs'], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config['dataloader']['test_bs'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = Encoder(**config['encoder_cfg'])\n",
    "decoder = Decoder(**config['decoder_cfg'])\n",
    "\n",
    "model = NMTModel(encoder, decoder).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), config['optim']['lr'])\n",
    "criterion = torch.nn.NLLLoss(ignore_index = ru_tokenizer.encode(['<PAD>'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, loader, criterion, epoch, log_step=200):\n",
    "    model.train()\n",
    "    loss_val = []\n",
    "    avg_loss = []\n",
    "    iter_step = 1\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        for key in batch.keys():\n",
    "            batch[key] = batch[key].to(device)\n",
    "        preds = model(batch)\n",
    "        preds = preds.permute(0, 2, 1)\n",
    "        loss = criterion(preds, batch['target_for_loss'])\n",
    "        avg_loss.append(loss.detach().item())\n",
    "        if iter_step % log_step == 0:\n",
    "            avg_loss_val = sum(avg_loss) / len(avg_loss)\n",
    "            print('epoch\\t{}\\t[{}/{}]\\tloss: {:4f}'.format(epoch, iter_step, len(loader), avg_loss_val))\n",
    "            avg_loss = []\n",
    "            loss_val.append(avg_loss_val)\n",
    "        iter_step += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "EPOCHS = 1\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_loss = train_epoch(model, optimizer, loader, criterion, epoch)\n",
    "    losses.extend(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sent):\n",
    "    model.eval()\n",
    "    sent = eng_tokenizer.encode(eng_tokenizer.tokenize(sent))\n",
    "    sent = torch.LongTensor([sent])\n",
    "    dec_sent = [0]\n",
    "    while dec_sent[-1] != 1:\n",
    "        dec = torch.LongTensor([dec_sent])\n",
    "        batch = {'source':sent, 'source_mask':None, 'target':dec, 'target_mask':None}\n",
    "        with torch.no_grad():\n",
    "            max_val = torch.exp(model(batch)).max(dim=2)[1][:,-1].item()\n",
    "        dec_sent.append(max_val)\n",
    "\n",
    "    print(' '.join(ru_tokenizer.decode(dec_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(model, 'i love cats')"
   ]
  }
 ]
}